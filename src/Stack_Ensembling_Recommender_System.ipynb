{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7708c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c994b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144970b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmotionModel:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "\n",
    "    def train(self, train_texts, train_labels, val_texts, val_labels, epochs=3, batch_size=16):\n",
    "        train_dataset = TextDataset(train_texts, train_labels, self.tokenizer)\n",
    "        val_dataset = TextDataset(val_texts, val_labels, self.tokenizer)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=1e-5)\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids']\n",
    "                attention_mask = batch['attention_mask']\n",
    "                labels = batch['label']\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    def predict(self, text):\n",
    "        self.model.eval()\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=encoding['input_ids'], attention_mask=encoding['attention_mask'])\n",
    "        return torch.softmax(outputs.logits, dim=1).numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4617f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CollaborativeFiltering:\n",
    "    def __init__(self, n_components=50):\n",
    "        self.n_components = n_components\n",
    "        self.svd = TruncatedSVD(n_components=self.n_components)\n",
    "\n",
    "    def fit(self, user_item_matrix):\n",
    "        self.user_factors = self.svd.fit_transform(user_item_matrix)\n",
    "        self.item_factors = self.svd.components_\n",
    "\n",
    "    def predict(self, user_index):\n",
    "        return np.dot(self.user_factors[user_index], self.item_factors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContentBasedFiltering:\n",
    "    def __init__(self, emotion_model):\n",
    "        self.emotion_model = emotion_model\n",
    "\n",
    "    def predict(self, user_feedback, item_descriptions):\n",
    "        user_emotion = self.emotion_model.predict(user_feedback)\n",
    "        item_emotions = np.array([self.emotion_model.predict(desc) for desc in item_descriptions])\n",
    "        return cosine_similarity(user_emotion.reshape(1, -1), item_emotions).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StackEnsemble:\n",
    "    def __init__(self):\n",
    "        self.meta_model = LogisticRegression()\n",
    "\n",
    "    def fit(self, base_predictions, y_train):\n",
    "        self.meta_model.fit(base_predictions, y_train)\n",
    "\n",
    "    def predict(self, base_predictions):\n",
    "        return self.meta_model.predict_proba(base_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RecommenderSystem:\n",
    "    def __init__(self, n_components=50):\n",
    "        self.collaborative_filtering = CollaborativeFiltering(n_components=n_components)\n",
    "        self.emotion_model = EmotionModel()\n",
    "        self.content_based_filtering = ContentBasedFiltering(self.emotion_model)\n",
    "        self.stack_ensemble = StackEnsemble()\n",
    "\n",
    "    def train_emotion_model(self, train_texts, train_labels, val_texts, val_labels, epochs=3, batch_size=16):\n",
    "        self.emotion_model.train(train_texts, train_labels, val_texts, val_labels, epochs, batch_size)\n",
    "\n",
    "    def fit(self, user_item_matrix, user_feedbacks, item_descriptions, y_train):\n",
    "        self.collaborative_filtering.fit(user_item_matrix)\n",
    "        collaborative_predictions = [self.collaborative_filtering.predict(i) for i in range(user_item_matrix.shape[0])]\n",
    "        content_predictions = [self.content_based_filtering.predict(feedback, item_descriptions) for feedback in user_feedbacks]\n",
    "\n",
    "        base_predictions = np.hstack((collaborative_predictions, content_predictions))\n",
    "        self.stack_ensemble.fit(base_predictions, y_train)\n",
    "\n",
    "    def recommend(self, user_index, user_feedback, item_descriptions):\n",
    "        collaborative_pred = self.collaborative_filtering.predict(user_index)\n",
    "        content_pred = self.content_based_filtering.predict(user_feedback, item_descriptions)\n",
    "        base_pred = np.hstack((collaborative_pred, content_pred)).reshape(1, -1)\n",
    "        final_pred = self.stack_ensemble.predict(base_pred)\n",
    "        return np.argsort(final_pred.flatten())[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b67c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulated data\n",
    "    user_item_matrix = np.random.rand(10, 50)\n",
    "    user_feedbacks = [\"I feel good about this product.\", \"I'm not happy with the quality.\", \"This product is excellent.\"]\n",
    "    item_descriptions = [\"A great product for daily use.\", \"Top-notch quality and performance.\", \"Affordable and reliable.\"]\n",
    "\n",
    "    train_texts = [\"I love this!\", \"Not satisfied\", \"Works well\"]\n",
    "    train_labels = [1, 0, 1]\n",
    "    val_texts = [\"Amazing product\", \"Disappointing experience\"]\n",
    "    val_labels = [1, 0]\n",
    "    y_train = np.random.randint(0, 2, size=10)\n",
    "\n",
    "    recommender = RecommenderSystem()\n",
    "    recommender.train_emotion_model(train_texts, train_labels, val_texts, val_labels)\n",
    "\n",
    "    recommender.fit(user_item_matrix, user_feedbacks, item_descriptions, y_train)\n",
    "    recommendations = recommender.recommend(user_index=0, user_feedback=\"I feel overwhelmed by the choices.\", item_descriptions=item_descriptions)\n",
    "    print(f\"Recommended item indices: {recommendations[:3]}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
